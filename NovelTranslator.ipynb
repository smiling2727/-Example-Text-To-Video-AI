{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO86ws95jI5mqrTrse61GYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smiling2727/-Example-Text-To-Video-AI/blob/main/NovelTranslator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxndTl_rCYE0",
        "outputId": "82aa72df-468b-4cf8-865d-f2dcddfdce81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_novel_drive_path = '/content/drive/MyDrive/video_generation/novel_translate/input' # @param {type:\"string\"}\n",
        "output_novel_drive_path = '/content/drive/MyDrive/video_generation/novel_translate/output' # @param {type:\"string\"}\n",
        "input_txt_filename='buerzhichen.txt' # @param {type:\"string\"}\n",
        "novel_name='buerzhichen' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "y_oVK02fCt5c"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install packages\n",
        "!pip install tqdm\n",
        "!pip install anthropic\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3MCwAunDyCF",
        "outputId": "417043f8-7a7d-4f62-8900-fcb4e871f8b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.39.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.4)\n",
            "Downloading anthropic-0.39.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.4/198.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.39.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import anthropic\n",
        "from openai import OpenAI\n",
        "# 点击colab做吧的钥匙(Secrets)按钮，并将CLAUDE_API_KEY加入列表中，勾选Notebook access\n",
        "CLAUDE_API_KEY = userdata.get(\"CLAUDE_API_KEY\")\n",
        "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "open_ai_client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "m3SViiYJDdqg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import anthropic\n",
        "from tqdm import tqdm\n",
        "\n",
        "def read_input_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "def sample_text(text, sample_size=5000, num_samples=30):\n",
        "    length = len(text)\n",
        "    samples = []\n",
        "    for i in range(num_samples):\n",
        "        start = (i * length) // num_samples\n",
        "        end = start + sample_size\n",
        "        samples.append(text[start:end])\n",
        "    return samples\n",
        "\n",
        "def extract_and_translate_terms(sample, previous_results=\"\"):\n",
        "    # 解析之前的结果，获取最后使用的编号\n",
        "    last_number = 0\n",
        "    existing_terms = {}\n",
        "    for line in previous_results.split('\\n'):\n",
        "        if line.strip():\n",
        "            match = re.match(r'(\\d+)\\.\\s(.+?):', line)\n",
        "            if match:\n",
        "                number, term = match.groups()\n",
        "                last_number = max(last_number, int(number))\n",
        "                existing_terms[term] = int(number)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    请从以下中文小说文本样本中提取并翻译以下类型的术语：\n",
        "    1. 人物名字\n",
        "    2. 家族名称\n",
        "    3. 地名（星球、国家、城市等）\n",
        "    4. 组织名称（军队、政府机构等）\n",
        "    5. 特殊术语（与故事世界观相关的概念）\n",
        "\n",
        "    重要规则：\n",
        "    1. 保持之前翻译结果中的编号不变。\n",
        "    2. 对于新的术语，从{last_number + 1}开始编号。\n",
        "    3. 如果某个术语之前已经翻译过，在描述中添加\"(之前已翻译)\"，但仍然提供完整的英文翻译。\n",
        "    4. 人名翻译：\n",
        "       - 为每个中文姓氏选择一个固定的英文姓氏，并在整个家族中保持一致。\n",
        "       - 例如，如果\"颜\"家族的一个成员被翻译为\"Evelyn Yance\"，那么所有颜家成员都应使用\"Yance\"作为姓氏。不要使用中文的拼音。\n",
        "       - 个人名字应该英语化，但可以保留一些原名的特点，应当改为英文名字，避免使用拼音。\n",
        "\n",
        "    5. 家族名称：\n",
        "       - 使用选定的英文姓氏+\"family\"。例如：\"颜家\" -> \"Yance family\"\n",
        "\n",
        "    6. 地名和组织名称：\n",
        "       - 可以适当音译或意译，但要保持其独特性和科幻感。\n",
        "       - 对于虚构的地名，可以创造性地翻译，但要确保其含义或特点得到保留。\n",
        "\n",
        "    7. 特殊术语：\n",
        "       - 应根据其含义进行翻译，同时注重保持科幻感和独特性。\n",
        "       - 对于关键概念（如\"alpha\"、\"omega\"、\"信息素\"等），保持一致的翻译。\n",
        "\n",
        "    8. 翻译风格一致性：\n",
        "       - 对于人名，要么全部英语化，要么在英语化的基础上保留一些原始发音的特点，但要在整个文本中保持一致。\n",
        "       - 例如，如果决定将\"冯\"翻译为\"Felix\"，那么其他类似的中文名字也应该采用类似的英语化方法。\n",
        "\n",
        "    9. 保持所有翻译的一致性：\n",
        "       - 如果某个术语之前已经翻译过，请在描述中添加\"(之前已翻译)\"，但仍然提供完整的英文翻译。\n",
        "       - 在描述中注明\"之前已翻译\"，以便跟踪和维护一致性。\n",
        "\n",
        "    请以以下格式输出结果：\n",
        "    编号. 中文术语 (English Translation): 简短描述和类型（人名/地名/组织/术语等）\n",
        "\n",
        "    之前的翻译结果：\n",
        "    {previous_results}\n",
        "\n",
        "    新文本样本：\n",
        "    {sample}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-5-sonnet-20240620\",\n",
        "        max_tokens=8000,\n",
        "        temperature=0.2,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.content[0].text\n",
        "\n",
        "def process_samples(samples):\n",
        "    all_results = \"\"\n",
        "    term_mapping = {}\n",
        "    for i, sample in enumerate(tqdm(samples, desc=\"Processing samples\")):\n",
        "        results = extract_and_translate_terms(sample, all_results)\n",
        "        all_results += results + \"\\n\\n\"\n",
        "\n",
        "        # 解析结果并更新术语映射\n",
        "        for line in results.split('\\n'):\n",
        "            if ':' in line:\n",
        "                number_and_term, description = line.split(':', 1)\n",
        "                match = re.match(r'(\\d+)\\.\\s(.+?)\\s*\\(([^)]+)\\)', number_and_term)\n",
        "                if match:\n",
        "                    number, chinese_term, english_term = match.groups()\n",
        "                    term_mapping[chinese_term] = {\n",
        "                        \"number\": number,\n",
        "                        \"english_term\": english_term.strip(),\n",
        "                        \"description\": description.strip()\n",
        "                    }\n",
        "\n",
        "    return term_mapping\n",
        "\n",
        "def save_term_mapping(term_mapping, novel_name, output_novel_drive_path):\n",
        "    output_file = f\"Name_mapping_{novel_name}.json\"\n",
        "    # output_path = os.path.join(output_novel_drive_path, output_file)\n",
        "    output_novel_name_path = os.path.join(output_novel_drive_path, novel_name)\n",
        "    os.makedirs(output_novel_name_path, exist_ok=True)\n",
        "    output_path = os.path.join(output_novel_name_path, output_file)\n",
        "    print(f\"Saving term mapping to {output_path}\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(term_mapping, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Term mapping saved to {output_path}\")\n",
        "\n",
        "\n",
        "def step_1_term_mapping_main(input_novel_drive_path, input_txt_filename, output_novel_drive_path, novel_name):\n",
        "    input_file = os.path.join(input_novel_drive_path, input_txt_filename)\n",
        "    print(f\"Input file path: {input_file}\")\n",
        "    print(\"Reading input file...\")\n",
        "    chinese_text = read_input_file(input_file)\n",
        "    print(\"Sampling text...\")\n",
        "    samples = sample_text(chinese_text)\n",
        "    print(\"Processing samples and generating term mapping...\")\n",
        "    term_mapping = process_samples(samples)\n",
        "    print(\"Saving term mapping...\")\n",
        "    save_term_mapping(term_mapping, novel_name, output_novel_drive_path)\n",
        "    print(\"Term mapping process completed.\")\n",
        "\n",
        "# def get_novel_name_mapping(input_novel_drive_path, input_txt_filename, output_novel_drive_path, novel_name):\n",
        "#    main(input_novel_drive_path, input_txt_filename, output_novel_drive_path, novel_name)\n",
        "\n",
        "step_1_term_mapping_main(input_novel_drive_path, input_txt_filename, output_novel_drive_path, novel_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scttu7YoEiSW",
        "outputId": "aae9ac37-2ed0-4a0a-949e-66edba79becc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file path: /content/drive/MyDrive/video_generation/novel_translate/input/buerzhichen.txt\n",
            "Reading input file...\n",
            "Sampling text...\n",
            "Processing samples and generating term mapping...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 100%|██████████| 30/30 [03:09<00:00,  6.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving term mapping...\n",
            "Saving term mapping to /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/Name_mapping_buerzhichen.json\n",
            "Term mapping saved to /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/Name_mapping_buerzhichen.json\n",
            "Term mapping process completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import anthropic\n",
        "import os\n",
        "\n",
        "def read_json_file(output_novel_drive_path, novel_name, file_name):\n",
        "    \"\"\"从小说专属文件夹读取JSON文件\"\"\"\n",
        "    novel_folder = os.path.join(output_novel_drive_path, novel_name)\n",
        "    print(f\"novel_folder = {novel_folder}\")\n",
        "    file_path = os.path.join(novel_folder, file_name)\n",
        "    print(f\"novel_file_path = {file_path}\")\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"在 {novel_folder} 中找不到文件 '{file_name}'。\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"'{file_name}' 不是有效的JSON文件。\")\n",
        "        return None\n",
        "\n",
        "def write_json_file(output_novel_drive_path, content, novel_name, file_name):\n",
        "    \"\"\"将JSON内容写入小说专属文件夹\"\"\"\n",
        "    novel_folder = os.path.join(output_novel_drive_path, novel_name)\n",
        "    # 确保文件夹存在\n",
        "    if not os.path.exists(novel_folder):\n",
        "        os.makedirs(novel_folder)\n",
        "    file_path = os.path.join(novel_folder, file_name)\n",
        "    try:\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(content, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"文件已成功保存为 '{file_path}'。\")\n",
        "    except IOError:\n",
        "        print(f\"无法写入文件 '{file_path}'。\")\n",
        "\n",
        "def clean_json_response(response_text):\n",
        "    \"\"\"清理和修复JSON响应中的常见问题\"\"\"\n",
        "    try:\n",
        "        # 移除可能的前缀和后缀文本\n",
        "        json_start = response_text.find('{')\n",
        "        json_end = response_text.rfind('}') + 1\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            response_text = response_text[json_start:json_end]\n",
        "\n",
        "        # 移除最后一项后的逗号\n",
        "        response_text = re.sub(r',(\\s*})', r'\\1', response_text)\n",
        "\n",
        "        # 确保所有字符串使用双引号\n",
        "        response_text = re.sub(r\"'([^']*)':\", r'\"\\1\":', response_text)\n",
        "        response_text = re.sub(r':\\s*\\'([^\\']*)\\'', r':\"\\1\"', response_text)\n",
        "\n",
        "        return response_text\n",
        "    except Exception as e:\n",
        "        print(f\"清理JSON时出错: {str(e)}\")\n",
        "        return response_text\n",
        "\n",
        "def validate_term_mapping(terms):\n",
        "    \"\"\"验证术语映射的格式和内容\"\"\"\n",
        "    try:\n",
        "        if not isinstance(terms, dict):\n",
        "            raise ValueError(\"术语映射必须是一个字典\")\n",
        "\n",
        "        for term, info in terms.items():\n",
        "            # 验证基本结构\n",
        "            if not isinstance(info, dict):\n",
        "                raise ValueError(f\"术语 '{term}' 的信息必须是一个字典\")\n",
        "\n",
        "            # 验证必需字段\n",
        "            required_fields = ['number', 'english_term', 'description']\n",
        "            for field in required_fields:\n",
        "                if field not in info:\n",
        "                    raise ValueError(f\"术语 '{term}' 缺少必需字段 '{field}'\")\n",
        "\n",
        "            # 验证number格式\n",
        "            if not str(info['number']).isdigit():\n",
        "                raise ValueError(f\"术语 '{term}' 的编号必须是数字\")\n",
        "\n",
        "            # 验证字段类型\n",
        "            if not isinstance(info['english_term'], str):\n",
        "                raise ValueError(f\"术语 '{term}' 的english_term必须是字符串\")\n",
        "            if not isinstance(info['description'], str):\n",
        "                raise ValueError(f\"术语 '{term}' 的description必须是字符串\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"验证失败: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def process_terms_with_claude(terms, output_novel_drive_path, novel_name):\n",
        "    terms_str = json.dumps(terms, ensure_ascii=False, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    请处理以下术语列表，遵循这些规则：\n",
        "    1. 按照英文术语（english_term）的字母顺序对所有条目进行排序。\n",
        "    2. 从1开始连续重新编号，更新\"number\"字段值。\n",
        "    3. 严格注意：对于人名，确保使用完全英语的名字，可以对json进行更改。必须必须避免中文名字的拼音，而是根据人物的特点和身份起一个新的英文名字和英文姓氏，并遵循英文姓名顺序（名在前，姓在后）。\n",
        "    4. 检查并改进所有翻译，确保准确性和一致性。\n",
        "    5. 保持每个术语的中文名（键名）不变。\n",
        "    6. 输出格式必须是有效的JSON，确保：\n",
        "       - 所有字符串都用双引号（\"）括起来\n",
        "       - 所有字符串都正确结束，不包含未转义的引号\n",
        "       - 所有括号都正确配对\n",
        "       - 所有值都用逗号正确分隔，最后一项不要有逗号\n",
        "    7. 确保所有原有的条目都包含在输出中，不要遗漏或添加额外的条目。\n",
        "    8. 直接返回JSON对象，不要添加任何额外的解释或说明。\n",
        "\n",
        "    原始术语列表：\n",
        "    {terms_str}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20240620\",\n",
        "            # model=\"claude-3-opus-latest\",\n",
        "            max_tokens=8000,\n",
        "            temperature=0.2,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # 清理JSON响应\n",
        "        cleaned_text = clean_json_response(response_text)\n",
        "\n",
        "        try:\n",
        "            # 解析JSON\n",
        "            processed_terms = json.loads(cleaned_text)\n",
        "\n",
        "            # 验证处理后的数据\n",
        "            if validate_term_mapping(processed_terms):\n",
        "                return processed_terms\n",
        "            else:\n",
        "                raise ValueError(\"术语映射验证失败\")\n",
        "\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            print(f\"JSON处理错误: {str(e)}\")\n",
        "\n",
        "            # 保存调试信息\n",
        "            debug_dir = os.path.join(os.path.dirname(os.path.abspath(output_novel_drive_path)), novel_name, \"debug\")\n",
        "            os.makedirs(debug_dir, exist_ok=True)\n",
        "\n",
        "            # 保存原始响应、清理后的响应和错误信息\n",
        "            with open(os.path.join(debug_dir, \"raw_response.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(response.content[0].text)\n",
        "\n",
        "            with open(os.path.join(debug_dir, \"cleaned_response.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(cleaned_text)\n",
        "\n",
        "            with open(os.path.join(debug_dir, \"error_info.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"错误类型: {type(e).__name__}\\n错误信息: {str(e)}\")\n",
        "\n",
        "            print(f\"调试文件已保存到 {debug_dir} 文件夹\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"处理术语时出现问题: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_terms_with_gpt(terms, output_novel_drive_path, novel_name):\n",
        "    terms_str = json.dumps(terms, ensure_ascii=False, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    请处理以下术语列表，遵循这些规则：\n",
        "    1. 按照英文术语（english_term）的字母顺序对所有条目进行排序。\n",
        "    2. 从1开始连续重新编号，更新\"number\"字段值。\n",
        "    3. 严格注意：对于人名，确保使用完全英语的名字，可以对json进行更改。必须必须避免中文名字的拼音，而是根据人物的特点和身份起一个新的英文名字和英文姓氏，并遵循英文姓名顺序（名在前，姓在后）。\n",
        "    4. 检查并改进所有翻译，确保准确性和一致性。\n",
        "    5. 保持每个术语的中文名（key）不变。并且输出格式跟原始术语列表格式一致。 例子 \"古董包\": {{\"number\": \"36\", \"english_term\": \"Vintage Bag\", \"description\": \"特殊术语,指具有收藏价值的旧包\"}}\n",
        "    6. 如果对翻译进行了改进，请在描述（description）字段末尾添加改进原因，格式为\"(改进原因: [原因])\"。\n",
        "    7. 输出格式必须是有效的JSON，确保：\n",
        "       - 所有字符串都用双引号（\"）括起来\n",
        "       - 所有字符串都正确结束，不包含未转义的引号\n",
        "       - 所有括号都正确配对\n",
        "       - 所有值都用逗号正确分隔，最后一项不要有逗号\n",
        "    8. 确保所有原有的都必须包含在输出中，不要遗漏或添加额外的条目。\n",
        "    9. 直接返回JSON对象，不要添加任何额外的解释或说明。\n",
        "\n",
        "    原始术语列表：\n",
        "    {terms_str}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # response = client.messages.create(\n",
        "        #     model=\"claude-3-5-sonnet-20240620\",\n",
        "        #     # model=\"claude-3-opus-latest\",\n",
        "        #     max_tokens=8000,\n",
        "        #     temperature=0.2,\n",
        "        #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        # )\n",
        "\n",
        "        completion = open_ai_client.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          response_format={\"type\": \"json_object\" },\n",
        "          messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "          ],\n",
        "        )\n",
        "\n",
        "        response_text = completion.choices[0].message.content\n",
        "\n",
        "        # 清理JSON响应\n",
        "        cleaned_text = clean_json_response(response_text)\n",
        "\n",
        "        try:\n",
        "            # 解析JSON\n",
        "            processed_terms = json.loads(cleaned_text)\n",
        "\n",
        "            # 验证处理后的数据\n",
        "            if validate_term_mapping(processed_terms):\n",
        "                return processed_terms\n",
        "            else:\n",
        "                raise ValueError(\"术语映射验证失败\")\n",
        "\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            print(f\"JSON处理错误: {str(e)}\")\n",
        "\n",
        "            # 保存调试信息\n",
        "            debug_dir = os.path.join(os.path.dirname(os.path.abspath(output_novel_drive_path)), novel_name, \"debug\")\n",
        "            os.makedirs(debug_dir, exist_ok=True)\n",
        "\n",
        "            # 保存原始响应、清理后的响应和错误信息\n",
        "            with open(os.path.join(debug_dir, \"raw_response.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(response.content[0].text)\n",
        "\n",
        "            with open(os.path.join(debug_dir, \"cleaned_response.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(cleaned_text)\n",
        "\n",
        "            with open(os.path.join(debug_dir, \"error_info.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"错误类型: {type(e).__name__}\\n错误信息: {str(e)}\")\n",
        "\n",
        "            print(f\"调试文件已保存到 {debug_dir} 文件夹\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"处理术语时出现问题: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def step_2_mapping_cleanup_main(output_novel_drive_path, novel_name):\n",
        "    input_file = f\"Name_mapping_{novel_name}.json\"\n",
        "    output_file = f\"Processed_mapping_{novel_name}.json\"\n",
        "\n",
        "    print(f\"正在从 {novel_name} 文件夹读取 {input_file}...\")\n",
        "    terms = read_json_file(output_novel_drive_path, novel_name, input_file)\n",
        "\n",
        "    if terms:\n",
        "        print(f\"读取到 {len(terms)} 个术语条目\")\n",
        "        # print(\"使用 Claude 处理术语...\")\n",
        "        # processed_terms = process_terms_with_claude(terms, output_novel_drive_path, novel_name)\n",
        "        print(\"使用 GPT 处理术语...\")\n",
        "        processed_terms = process_terms_with_gpt(terms, output_novel_drive_path, novel_name)\n",
        "        if processed_terms:\n",
        "            print(f\"成功处理 {len(processed_terms)} 个术语条目\")\n",
        "            print(\"写入输出文件...\")\n",
        "            write_json_file(output_novel_drive_path, processed_terms, novel_name, output_file)\n",
        "        else:\n",
        "            print(\"无法处理术语。请检查调试文件夹中的响应内容。\")\n",
        "    else:\n",
        "        print(\"无法处理输入文件。请确保文件存在且格式正确。\")\n",
        "    print(\"处理完成。\")\n",
        "\n",
        "\n",
        "step_2_mapping_cleanup_main(output_novel_drive_path, novel_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPLyzEhKJ0Av",
        "outputId": "ab150ca8-b6c7-443a-c904-67b6e25461c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在从 buerzhichen 文件夹读取 Name_mapping_buerzhichen.json...\n",
            "novel_folder = /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen\n",
            "novel_file_path = /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/Name_mapping_buerzhichen.json\n",
            "读取到 174 个术语条目\n",
            "使用 GPT 处理术语...\n",
            "成功处理 34 个术语条目\n",
            "写入输出文件...\n",
            "文件已成功保存为 '/content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/Processed_mapping_buerzhichen.json'。\n",
            "处理完成。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import anthropic\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "from openai import RateLimitError, APIError\n",
        "\n",
        "TRANSLATION_FOLDER = \"translation\"\n",
        "def read_file(file_path):\n",
        "    \"\"\"从小说专属文件夹读取文件\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"找不到文件 '{file_path}'\")\n",
        "        return None\n",
        "\n",
        "def write_chapter_file(output_novel_drive_path, content, chapter_number, novel_name):\n",
        "    \"\"\"将翻译后的章节保存到小说专属文件夹的Translated_Novelname子文件夹中\"\"\"\n",
        "    novel_folder = os.path.join(output_novel_drive_path, novel_name)\n",
        "    translation_folder = os.path.join(novel_folder, TRANSLATION_FOLDER)\n",
        "    print(\"translation_folder\", translation_folder)\n",
        "    print('translation_folder: ', translation_folder)\n",
        "    # 确保翻译文件夹存在\n",
        "    if not os.path.exists(translation_folder):\n",
        "        os.makedirs(translation_folder)\n",
        "        print(f\"创建翻译文件夹: {translation_folder}\")\n",
        "\n",
        "    file_name = f\"chapter_{chapter_number:03d}.txt\"\n",
        "    file_path = os.path.join(translation_folder, file_name)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "    print(f\"已保存: {file_name}\")\n",
        "\n",
        "def load_term_mapping(output_novel_drive_path, novel_name):\n",
        "    \"\"\"从小说专属文件夹加载术语映射\"\"\"\n",
        "    novel_folder = os.path.join(output_novel_drive_path, novel_name)\n",
        "    file_name = f\"Processed_mapping_{novel_name}.json\"\n",
        "    file_path = os.path.join(novel_folder, file_name)\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"在 {novel_folder} 中找不到文件 '{file_name}'\")\n",
        "        return None\n",
        "\n",
        "def split_by_chapters(text):\n",
        "    chapter_pattern = re.compile(r'^第\\d+章', re.MULTILINE)\n",
        "    chapter_starts = [m.start() for m in chapter_pattern.finditer(text)]\n",
        "    chapters = []\n",
        "    for i in range(len(chapter_starts)):\n",
        "        start = chapter_starts[i]\n",
        "        end = chapter_starts[i+1] if i+1 < len(chapter_starts) else None\n",
        "        chapter_content = text[start:end]\n",
        "        chapters.append(chapter_content.strip())\n",
        "    return chapters\n",
        "\n",
        "def process_text(text):\n",
        "    first_chapter_start = re.search(r'^第\\d+章', text, re.MULTILINE)\n",
        "    if first_chapter_start:\n",
        "        prefix = text[:first_chapter_start.start()].strip()\n",
        "        main_content = text[first_chapter_start.start():]\n",
        "        chapters = split_by_chapters(main_content)\n",
        "        if prefix:\n",
        "            chapters.insert(0, prefix)\n",
        "    else:\n",
        "        chapters = [text]\n",
        "    return chapters\n",
        "\n",
        "\n",
        "def translate_chapter_by_gpt(chapter, term_mapping, open_ai_client, max_retries=3):\n",
        "    term_mapping_str = json.dumps(term_mapping, ensure_ascii=False, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Please translate the following Chinese text into English. Use the provided term mapping to ensure consistency of proper nouns.\n",
        "    Maintain the original paragraph structure and ensure accuracy and fluency in the translation.\n",
        "\n",
        "    Term Mapping:\n",
        "    {term_mapping_str}\n",
        "\n",
        "    Chinese Text:\n",
        "    {chapter}\n",
        "\n",
        "    Please provide the English translation. Provide only the translated text in English without adding any additional content or commentary. Ensure that no information is omitted from the original text.\n",
        "    \"\"\"\n",
        "\n",
        "    for retry_group in range(2):  # Two groups of retries\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # Send the API request\n",
        "                response = open_ai_client.chat.completions.create(\n",
        "                    model=\"gpt-4o-mini\",  # Replace with your model name\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt},\n",
        "                    ],\n",
        "                )\n",
        "\n",
        "                # Debugging: Print the response structure\n",
        "                print(\"Response:\", response)\n",
        "\n",
        "                # Extract the content from the response object\n",
        "                response_text = response.choices[0].message.content\n",
        "                return response_text\n",
        "\n",
        "            except RateLimitError:\n",
        "                print(f\"Rate limit error encountered. Retrying... (Group {retry_group + 1}, Attempt {attempt + 1}/{max_retries})\")\n",
        "            except APIError as e:\n",
        "                print(f\"API error: {str(e)}\")\n",
        "                return None\n",
        "            except Exception as e:\n",
        "                print(f\"Error during translation: {str(e)}\")\n",
        "                return None\n",
        "\n",
        "        if retry_group == 0:\n",
        "            print(\"First 3 attempts failed, waiting 30 seconds before next group...\")\n",
        "            time.sleep(10)  # Wait 30 seconds between retry groups\n",
        "\n",
        "    print(f\"Translation failed after {max_retries * 2} attempts.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def translate_chapter(chapter, term_mapping, client, max_retries=3):\n",
        "    term_mapping_str = json.dumps(term_mapping, ensure_ascii=False, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    请将以下中文文本翻译成英文。使用提供的术语映射来确保专有名词的一致性。\n",
        "    保持原文的段落结构，并确保翻译的准确性和流畅性。\n",
        "\n",
        "    术语映射：\n",
        "    {term_mapping_str}\n",
        "\n",
        "    中文文本：\n",
        "    {chapter}\n",
        "\n",
        "    Translate the Chinese text to English.  Provide only the translated text in English without adding any additional content or commentary. Ensure that no information is omitted from the original text.\n",
        "    \"\"\"\n",
        "\n",
        "    for retry_group in range(2):  # 两组尝试\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = client.messages.create(\n",
        "                    model=\"claude-3-5-sonnet-20240620\",\n",
        "                    max_tokens=8000,\n",
        "                    temperature=0.2,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                )\n",
        "                return response.content[0].text\n",
        "            except anthropic.APIError as e:\n",
        "                if e.status_code == 500:\n",
        "                    print(f\"遇到服务器错误，正在重试... (组 {retry_group + 1}, 尝试 {attempt + 1}/{max_retries})\")\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(5)  # 每次尝试之间等待5秒\n",
        "                else:\n",
        "                    print(f\"API错误: {str(e)}\")\n",
        "                    return None\n",
        "            except Exception as e:\n",
        "                print(f\"翻译时出错: {str(e)}\")\n",
        "                return None\n",
        "\n",
        "        if retry_group == 0:\n",
        "            print(\"前3次尝试失败，等待30秒后将进行下一组尝试...\")\n",
        "            time.sleep(30)  # 在两组尝试之间等待30秒\n",
        "\n",
        "    print(f\"在 {max_retries * 2} 次尝试后翻译失败\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def step_3_main(input_novel_drive_path, input_txt_filename, output_novel_drive_path, novel_name):\n",
        "    # 使用与前面脚本相同的文件名格式\n",
        "    # term_mapping_file = f\"Processed_mapping_{novel_name}.json\"\n",
        "    input_file_path = os.path.join(input_novel_drive_path, input_txt_filename)\n",
        "    print(\"input_novel_drive_path: \", input_novel_drive_path)\n",
        "    print(f\"Input file path: {input_file_path}\")\n",
        "    print(\"读取原文...\")\n",
        "\n",
        "    original_text = read_file(input_file_path)\n",
        "    if not original_text:\n",
        "        print(\"无法读取原文文件，程序终止。\")\n",
        "        return\n",
        "\n",
        "    print(\"加载术语映射...\")\n",
        "    term_mapping = load_term_mapping(output_novel_drive_path, novel_name)\n",
        "    if not term_mapping:\n",
        "        print(\"无法加载术语映射文件，程序终止。\")\n",
        "        return\n",
        "\n",
        "    print(\"分割章节...\")\n",
        "    chapters = process_text(original_text)\n",
        "    print(f\"总共分割出 {len(chapters)} 章\")\n",
        "\n",
        "    # 输出文件夹路径\n",
        "    # current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    output_folder = os.path.join(output_novel_drive_path, novel_name, TRANSLATION_FOLDER)\n",
        "    os.makedirs(output_folder, exist_ok=True) # create TRANSLATION_FOLDER\n",
        "    print(f\"翻译文件将保存到: {output_folder}\")\n",
        "\n",
        "    for i, chapter in enumerate(tqdm(chapters, desc=\"翻译进度\")):\n",
        "        print(f\"\\n翻译第 {i+1} 章...\")\n",
        "        translated_chapter = translate_chapter_by_gpt(chapter, term_mapping, open_ai_client)\n",
        "        # translated_chapter = translate_chapter(chapter, term_mapping, client)\n",
        "        if translated_chapter:\n",
        "            # write_chapter_file(output_novel_drive_path, content, chapter_number, novel_name):\n",
        "            write_chapter_file(output_novel_drive_path, translated_chapter, i+1, novel_name)\n",
        "        else:\n",
        "            print(f\"第 {i+1} 章翻译失败，跳过\")\n",
        "\n",
        "    print(f\"翻译完成！所有章节已保存到 {novel_name}/{TRANSLATION_FOLDER} 文件夹\")\n",
        "\n",
        "\n",
        "step_3_main(input_novel_drive_path, input_txt_filename, output_novel_drive_path, novel_name )"
      ],
      "metadata": {
        "id": "uzzFNGcjJK0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c4c3f3-0405-4b4b-8de2-2068a2863ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_novel_drive_path:  /content/drive/MyDrive/video_generation/novel_translate/input\n",
            "Input file path: /content/drive/MyDrive/video_generation/novel_translate/input/buerzhichen.txt\n",
            "读取原文...\n",
            "加载术语映射...\n",
            "分割章节...\n",
            "总共分割出 89 章\n",
            "翻译文件将保存到: /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/translation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r翻译进度:   0%|          | 0/89 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "翻译第 1 章...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r翻译进度:   1%|          | 1/89 [00:00<00:58,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: ChatCompletion(id='chatcmpl-AX3YrujBG3HajBMcgLFhAnEGIKy7j', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"The Unsurpassable Minister\"', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732441877, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=7, prompt_tokens=7213, total_tokens=7220, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
            "translation_folder /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/translation\n",
            "translation_folder:  /content/drive/MyDrive/video_generation/novel_translate/output/buerzhichen/translation\n",
            "已保存: chapter_001.txt\n",
            "\n",
            "翻译第 2 章...\n"
          ]
        }
      ]
    }
  ]
}